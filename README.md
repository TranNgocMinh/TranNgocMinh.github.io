# EmoNeXt++-Tiny for FER2013

This repository implements **EmoNeXt++-Tiny**, a lightweight but powerful facial expression recognition model that extends ConvNeXt with STN, SE block, CLIP-based embedding supervision, and facial expression synthesis (FES).

## ğŸ“¦ File Structure

```bash
â”œâ”€â”€ models.py               # EmoNeXt++-Tiny architecture
â”œâ”€â”€ losses.py               # Custom loss functions
â”œâ”€â”€ train_emonextpp.py      # Training loop
â”œâ”€â”€ inference.py            # Inference from image
â”œâ”€â”€ evaluate.py             # Accuracy, F1 evaluation
â”œâ”€â”€ data_utils.py           # Dataset class for FER2013-style folder
â””â”€â”€ README.md               # Usage guide
```

## ğŸ—‚ï¸ Dataset Format (FER2013)
Prepare the FER2013 dataset in the following format:

```
./data/train/
â”œâ”€â”€ angry/
â”œâ”€â”€ disgust/
â”œâ”€â”€ fear/
â”œâ”€â”€ happy/
â”œâ”€â”€ sad/
â”œâ”€â”€ surprise/
â””â”€â”€ neutral/

./data/test/  (same structure)
```

Each subfolder contains images belonging to that emotion class.

## ğŸ‹ï¸â€â™€ï¸ Training
```bash
python train_emonextpp.py
```
This will train the model with CLIP supervision and self-contrastive loss.

## ğŸ§ª Evaluation
```bash
python evaluate.py
```
Will output accuracy, F1 score, and classification report on `./data/test`.

## ğŸ” Inference
```bash
python inference.py
```
Ensure you have a test image (`test.jpg`) and a trained model at `best_model.pth`.

## ğŸ’¡ Features
- ConvNeXt-Tiny backbone
- Spatial Transformer Network (STN)
- Squeeze-and-Excitation block
- CLIP-based embedding for better generalization
- Facial Expression Synthesis for class imbalance
- Multi-loss strategy: CE + Contrastive + Recon + Attention regularization

## ğŸ§  Citation & Base
Inspired by:
- EmoNeXt [https://github.com/yelboudouri/EmoNeXt]
- CLIP [https://openai.com/clip]

## ğŸ›  Requirements
- Python 3.10
- PyTorch >= 2.1.1
- torchvision
- transformers
- scikit-learn

## ğŸ“ License
MIT
